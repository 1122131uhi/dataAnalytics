{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 3a.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w9sEQx-NFJv"
      },
      "source": [
        "In this tutorial we will train and test a linear regressor. This is something you have already done in part 3 of the course document. So, I will miss out all of the extra notes and just get to business."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p-7cPGENClE"
      },
      "source": [
        "# needed to create the data frame\n",
        "import pandas as pd\n",
        "\n",
        "# needed to help with speedy maths based calculations\n",
        "import numpy as np\n",
        "\n",
        "# create data frame from csv file we hosted on our github\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/1122131uhi/dataAnalytics/master/tutorial2lineardata.csv', index_col=0, )"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEw-Y4JWNkh1",
        "outputId": "b25a36e2-c6ec-4f5c-c3df-30d3fe0f92d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# make sure we have our data by printing it out\n",
        "print(df[:6])\n",
        "# print(df) #all"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  temp  wdsp  NUM_TRIPS\n",
            "2     6  20.0   7.4   0.631126\n",
            "3     7  30.0  10.8   0.724951\n",
            "6     3  30.4   5.7   0.716044\n",
            "8     5  37.9  16.4   0.799994\n",
            "9     6  25.7  13.8   0.872611\n",
            "10    7  21.6   6.9   0.809792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTMxwMfGPdWJ"
      },
      "source": [
        "# A scale is not required here, but the constant will be useful in the assignment.\n",
        "SCALE_NUM_TRIPS = 1.0"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "5DzJzm8VY1rG",
        "outputId": "14c4afb5-c0e9-4fe7-a0f3-095dbb23d515",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataframe with the inputs and the output at the end using the imported dataframe. This can be replicated for any configuration, in this case, I have gone for day, temp, wdsp\n",
        "df_input_data = [df[\"day\"], df[\"temp\"], df[\"wdsp\"], df[\"NUM_TRIPS\"]] \n",
        "# create headers for our new dataframe. These should correlate with the above.\n",
        "df_input_headers = [\"day\", \"temp\", \"wdsp\", \"NUM_TRIPS\"] \n",
        "# create a final dataframe using our new dataframe and headers. \n",
        "df_input = pd.concat(df_input_data, axis=1, keys=df_input_headers) "
      ],
      "metadata": {
        "id": "WHuSnr6LdC3J"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a training set for runnign through the model and a test set, we do this by using sample with 0.8 for an 80% training set and 20% for test.\n",
        "training_set = df_input.sample(frac=0.8, random_state=0)\n",
        "test_set = df_input.drop(training_set.index)"
      ],
      "metadata": {
        "id": "d5JtvdLBcruq"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the datasets and remove the final column, i.e. the output column. We do this using pop.\n",
        "training_features = training_set.copy()\n",
        "test_features = test_set.copy()\n",
        "\n",
        "training_labels = training_features.pop('NUM_TRIPS')\n",
        "test_labels = test_features.pop('NUM_TRIPS')"
      ],
      "metadata": {
        "id": "EILvYoDid0wI"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I have put in a scale factor and divided by it. In this dataset, I had already normalised and thus it is 1. However, 600000 is what would make sense based on the data here and we can use this later when testing our model.. \n",
        "training_labels = training_labels/SCALE_NUM_TRIPS\n",
        "test_labels = test_labels/SCALE_NUM_TRIPS"
      ],
      "metadata": {
        "id": "3NBSa-Wyf2r6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# boiler plate for this model. You can see that we have used the training_features here for our normalisation layer that we try and fit to the outputs.\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features))"
      ],
      "metadata": {
        "id": "VQuNfPpdf9al"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I have decided to call the model, model_1. We add our normaliser and we are expecting a single output. \n",
        "model_1 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])"
      ],
      "metadata": {
        "id": "JKiP5GIrgIJH"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# more boiler plate for creating a sequential model, we need an optimiser and loss parameter. Here we are going to be using the mean absolute error MAE\n",
        "model_1.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "metadata": {
        "id": "4odNLaUHgNb7"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we are going to fit the model where we require the training features and labels. We will run it 100 times i.e. epochs and we have applied a further 20% validation split. \n",
        "\n",
        "%%time\n",
        "history = model_1.fit(\n",
        "    training_features,\n",
        "    training_labels,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "id": "rW1K3GxUgQyC",
        "outputId": "61a47ffb-d99d-4cf5-a9ce-8653dad19a6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.88 s, sys: 302 ms, total: 6.18 s\n",
            "Wall time: 6.38 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we will evaluate our model using the test features and labels. \n",
        "mean_absolute_error_model_1 = model_1.evaluate(\n",
        "    test_features,\n",
        "    test_labels, verbose=0)"
      ],
      "metadata": {
        "id": "ALdhLDb_gSjq"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The mean absolute error of the model can be printed out. Remember, we want to minimise this. Perhaps a model with just day and NUM_TRIPS would be better. It will also vary on each training run due to randomisation.\n",
        "print(mean_absolute_error_model_1)"
      ],
      "metadata": {
        "id": "NVLVxc_aghcD",
        "outputId": "2ba4015b-8b35-4580-c177-cd28e3ca6ea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05905775725841522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we create a custom dataframe with 3 values per feature. \n",
        "input_1 = pd.DataFrame.from_dict(data = \n",
        "\t\t\t\t{\n",
        "            'day' : [1,1,1],\n",
        "            'temp' : [61.8, 31.2, 77.0],\n",
        "            'wdsp' : [5.0, 3.0, 8.0]\n",
        "        })"
      ],
      "metadata": {
        "id": "hdG-BTFwhNT1"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next we can check this out, you can multiply by 600000 to get more realistic NUM_TRIPS values. \n",
        "linear_day_predictions_1 = model_1.predict(input_1[:3])*SCALE_NUM_TRIPS # essentially 600000 in this instance would give back realistic numbers based on the TAXI_TRIPS data\n",
        "print(linear_day_predictions_1)"
      ],
      "metadata": {
        "id": "bk96l6IfjA_1",
        "outputId": "1d1bd563-2610-4d12-ec1e-a9cd91bc2bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "[[0.6537206]\n",
            " [0.6791478]\n",
            " [0.6391644]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From test inputs we have used, the first element in the array here is similar to this actual data:\n",
        "\n",
        "\"165\",1,62.4,5.6,0.668458757342322\n",
        "\n",
        "We can see that the temperature is slightly higher in the test data (which means less trips, but slightly higher wind means more trips. So, the difference between (actual) 0.668 and (predicted) 0.576 (rounded to 3 significant figures) seems reasonable.\n",
        "\n",
        "Similarly with the second:\n",
        "\n",
        "\"389\",1,26.6,3.1,0.763954173062719, which has higher number of trips due to a lower temperature and also with a slightly higher wind speed.\n",
        "\n",
        "And with the third:\n",
        "\n",
        "\"571\",1,77.2,8.4,0.724652060408235\n",
        "\n",
        "The last prediction with the higher temperature seems to punish the values more."
      ],
      "metadata": {
        "id": "MZNN3Pdbj1I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same as above\n",
        "input_2 = pd.DataFrame.from_dict(data = \n",
        "\t\t\t\t{\n",
        "            'day' : [6,6,6],\n",
        "            'temp' : [61.8, 31.2, 77.0],\n",
        "            'wdsp' : [5.0, 3.0, 8.0]\n",
        "        })"
      ],
      "metadata": {
        "id": "1JBbhB3_kAZ0"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_day_predictions_2 = model_1.predict(input_2[:3])*SCALE_NUM_TRIPS # essentially 600000 in this instance would give back realistic numbers based on the TAXI_TRIPS data\n",
        "print(linear_day_predictions_2)"
      ],
      "metadata": {
        "id": "7wxV7TMMkJYn",
        "outputId": "496b59fc-d7a3-4883-d30d-6b66b86bbdb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[0.8802397]\n",
            " [0.9056669]\n",
            " [0.8656835]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test uses day 6 (Friday) instead of day 1 (Sunday) which shows higher number of trips. The other values were left the same."
      ],
      "metadata": {
        "id": "03hutuiGkR8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things to think about for the assignment. Make a validation set i.e. 5% of the data (or maybe more). This should be used for this type of testing. My values are simply made up.\n",
        "\n",
        "You should also remember to use different models with different data. In this case, I would maybe take each input valuable separately and make a regression model for each, then different variations i.e. any 2.\n",
        "\n",
        "Remember, you need to write up your results in the assignment. "
      ],
      "metadata": {
        "id": "0Mq41NEMkdqb"
      }
    }
  ]
}